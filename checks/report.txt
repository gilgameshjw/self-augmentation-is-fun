# Research Report: Startups Building New AI Architectures

## Introduction

The artificial intelligence (AI) landscape is undergoing a transformative phase, with startups at the forefront of innovation. While transformer-based architectures have dominated the AI field since the seminal 2017 paper *Attention Is All You Need*, a new wave of startups is exploring alternative architectures to address the limitations of transformers, such as high computational costs, inefficiency in certain tasks, and scalability challenges. These startups are experimenting with novel approaches, including diffusion models, liquid neural networks, and brain-inspired systems, to push the boundaries of AI capabilities. This report provides an in-depth analysis of startups building new AI architectures, highlighting their innovations, strategies, and potential impact on the industry.

---

## The Need for New AI Architectures

Transformer-based models like GPT and BERT have revolutionized natural language processing (NLP) and generative AI. However, their limitations, including quadratic computational costs, inefficiency in handling long context lengths, and reliance on massive datasets, have spurred interest in alternative architectures. Startups are addressing these challenges by developing architectures that are more efficient, adaptable, and specialized for diverse applications ([Digvijay, 2024](https://medium.com/@digvijay.qi/alternatives-to-transformer-based-architectures-3f41faeaacab); [Cheah, 2023](https://substack.tech-talk-cto.com/p/introducing-rwkv-an-alternative-to)).

---

## Startups Exploring New AI Architectures

### 1. **Liquid AI**
Liquid AI, a Massachusetts Institute of Technology (MIT) spinoff, has introduced *Liquid Foundation Models (LFMs)*, which deviate from the transformer architecture. These models are designed to be multimodal and efficient, focusing on building from "first principles" rather than relying on pre-trained transformers. Liquid AI's approach aims to create smaller, faster, and more cost-effective models that can handle diverse tasks ([VentureBeat, 2025](https://venturebeat.com/ai/mit-spinoff-liquid-debuts-non-transformer-ai-models-and-theyre-already-state-of-the-art/)).

Key Features:
- Multimodal capabilities for handling text, images, and other data types.
- Focus on efficiency and cost reduction.
- Designed for adaptability across various applications.

---

### 2. **Inception**
Inception, founded by Stanford computer science professor Stefano Ermon, has developed diffusion-based large language models (DLMs). Unlike transformers, DLMs leverage diffusion technology, which has been primarily used in image and video generation. Inception claims that its DLMs are up to 10 times faster and 10 times cheaper than traditional large language models (LLMs) ([TechCrunch, 2025](https://techcrunch.com/2025/02/26/inception-emerges-from-stealth-with-a-new-type-of-ai-model/)).

Key Innovations:
- Diffusion-based architecture for text generation.
- Optimized for efficient GPU utilization.
- Deployment options include APIs, on-premises, and edge devices.

---

### 3. **RWKV**
RWKV is an emerging alternative to transformers that combines the strengths of recurrent neural networks (RNNs) and transformers. This architecture addresses the quadratic computational costs of transformers, making it more efficient for handling long sequences. RWKV is particularly suited for tasks requiring extended context lengths ([Cheah, 2023](https://substack.tech-talk-cto.com/p/introducing-rwkv-an-alternative-to)).

Key Benefits:
- Linear computational costs, enabling scalability.
- Efficient handling of long-context tasks.
- Open-source development model fostering community contributions.

---

### 4. **Sakana AI**
Sakana AI, founded by one of the authors of the seminal Google research paper on transformers, is exploring "nature-inspired" AI architectures. The company recently released three Japanese-language models and is focusing on neural attention memory models (NAMMs). These models aim to enhance the efficiency and transferability of transformers ([CB Insights, 2024](https://cbi-www.lighthouse.ai/research/report/artificial-intelligence-top-startups-2024/)).

Key Features:
- Nature-inspired design principles.
- Focus on efficiency and adaptability.
- High valuation per employee, reflecting its innovative potential.

---

### 5. **DeepSeek**
DeepSeek has developed the R1 reasoning model, which offers performance comparable to OpenAI's o1 model at just 2% of the cost. R1 is optimized for older hardware and uses novel reinforcement learning techniques to reduce training costs. This approach democratizes access to advanced AI capabilities, making it accessible to startups and smaller organizations ([Forbes, 2025](https://www.forbes.com/sites/garydrenik/2025/02/26/how-ai-startups-are-evaluating-the-latest-model-advancements/)).

Key Innovations:
- Cost-effective model training and deployment.
- Optimized for legacy hardware.
- Focus on reasoning and decision-making tasks.

---

### 6. **Other Notable Startups**
- **Hum**: Focuses on architectures for scientific multimodal data and deliberate reasoning systems. Hum's models aim to transition from reactive responses to more human-like reasoning ([Hum, 2023](https://blog.hum.works/posts/trending-in-ai-new-architectures-and-deeper-reasoning)).
- **Anduril**: Specializes in defense technology, leveraging AI for autonomous systems and situational awareness ([Forbes, 2024](https://www.forbes.com/lists/ai50/)).
- **Figure AI**: Develops humanoid robots powered by advanced AI architectures ([Forbes, 2024](https://www.forbes.com/lists/ai50/)).

---

## Emerging Trends in AI Architectures

### 1. **Hybrid Approaches**
Startups are combining different architectures to leverage their strengths. For example, transformer-CNN hybrids are being used for tasks like image captioning, while transformer-RNN hybrids are suited for sequential data processing ([Digvijay, 2024](https://medium.com/@digvijay.qi/alternatives-to-transformer-based-architectures-3f41faeaacab)).

### 2. **Brain-Inspired Models**
Brain-inspired architectures aim to replicate the learning mechanisms of the human brain. These models promise superior performance and efficiency for complex tasks ([Digvijay, 2024](https://medium.com/@digvijay.qi/alternatives-to-transformer-based-architectures-3f41faeaacab)).

### 3. **Quantum Machine Learning**
Quantum computing is emerging as a potential game-changer for AI architectures. Quantum machine learning could revolutionize the field by offering new paradigms for problem-solving ([Digvijay, 2024](https://medium.com/@digvijay.qi/alternatives-to-transformer-based-architectures-3f41faeaacab)).

---

## Challenges and Opportunities

### Challenges
- **High R&D Costs**: Developing new architectures requires significant investment in research and development.
- **Market Adoption**: Convincing enterprises to adopt novel architectures over established transformers can be challenging.
- **Regulatory Hurdles**: Ensuring compliance with ethical and data privacy standards is critical.

### Opportunities
- **Efficiency Gains**: New architectures promise to reduce computational costs and improve scalability.
- **Specialized Applications**: Startups can target niche markets with architectures tailored to specific tasks.
- **Democratization of AI**: Cost-effective models like DeepSeek's R1 make advanced AI accessible to smaller organizations.

---

## Conclusion

Startups are playing a pivotal role in shaping the future of AI by developing innovative architectures that address the limitations of transformers. Companies like Liquid AI, Inception, RWKV, and Sakana AI are pioneering new approaches that promise to enhance efficiency, scalability, and adaptability. As the AI ecosystem continues to evolve, these startups are likely to drive the next wave of breakthroughs, enabling more versatile and impactful applications across industries.

---

## References

- VentureBeat. (2025, February 26). MIT spinoff Liquid debuts small, efficient non-transformer AI models. VentureBeat. https://venturebeat.com/ai/mit-spinoff-liquid-debuts-non-transformer-ai-models-and-theyre-already-state-of-the-art/
- TechCrunch. (2025, February 26). Inception emerges from stealth with a new type of AI model. TechCrunch. https://techcrunch.com/2025/02/26/inception-emerges-from-stealth-with-a-new-type-of-ai-model/
- Substack Tech Talk CTO. (2023, September 4). Introducing RWKV - an alternative to transformers - and why alternatives matter. https://substack.tech-talk-cto.com/p/introducing-rwkv-an-alternative-to
- CB Insights. (2024). AI 100: The most promising artificial intelligence startups of 2024. https://cbi-www.lighthouse.ai/research/report/artificial-intelligence-top-startups-2024/
- Forbes. (2025, February 26). How AI startups are evaluating the latest model advancements. Forbes. https://www.forbes.com/sites/garydrenik/2025/02/26/how-ai-startups-are-evaluating-the-latest-model-advancements/
- Medium. (2024, February 12). Alternatives to transformer-based architectures. Medium. https://medium.com/@digvijay.qi/alternatives-to-transformer-based-architectures-3f41faeaacab
- Hum. (2023). Trending in AI: New architectures and deeper reasoning. Hum. https://blog.hum.works/posts/trending-in-ai-new-architectures-and-deeper-reasoning
- Forbes. (2024). Forbes 2024 AI 50 List - Top Artificial Intelligence Startups. https://www.forbes.com/lists/ai50/